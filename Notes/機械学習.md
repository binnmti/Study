[機械学習集中講座](https://developers.google.com/machine-learning/crash-course?hl=ja)

# 機械学習の概要
- 機械学習(ML)は、モデルと呼ばれるソフトウェアをトレーニングして、有用な予測を行うか、データからコンテンツを生成するプロセス
  - たとえば、降雨を予測するアプリを作成するする
  - 従来のアプローチでは、地球の大気と地表を物理ベースで表現し、膨大な量の流体力学方程式を計算する。これは非常に困難
  - MLのアプローチでは、ML モデルに膨大な量の気象データを提供して、異なる量の雨をもたらす気象パターン間の数学的な関係を学習する。次にモデルに現在の天気データを渡すと、降水量が予測される。

## ML システムの種類
- 教師あり学習
- 教師なし学習
- 強化学習
- 生成 AI
 
## 教師あり学習モデル
- 正解を含む大量のデータを見て、正解を生成するデータ内の要素間の関連性を発見した後に予測を行うことができる。
- 過去の試験で十分にトレーニングを積んだら、新しい試験を受ける準備が整う。
- これらの ML システムは、人間が既知の正しい結果を含むデータを ML システムに提供するという意味で「教師あり」。
- 教師あり学習の最も一般的なユースケースの 2 つは、回帰と分類。
- 回帰モデルは数値を予測。たとえば、雨量を予測する天気モデル
- 分類モデルは、あるものがカテゴリに属する可能性で予測。たとえば、メールがスパムかどうか、写真に猫が写っているかどうか
- 分類モデルは、バイナリ分類とマルチクラス分類の 2 つのグループ。
- バイナリ分類モデルは、2 つの値のみを含むクラスから値を出力。たとえば、rain または no rain のいずれかを出力。
- マルチクラス分類モデルは、2 つ以上の値を含むクラスから値を出力。たとえば、rain、hail、snow、sleet のいずれかを出力。

## 教師なし学習
- 正しい回答が含まれていないデータを与えることで予測を行う。
- 目標は、データ内の有意なパターンを特定することです。つまり、独自のルールを推論する必要がある。
- よく使用される教師なし学習モデルでは、クラスタリングという手法が使用される。モデルは、自然なグループを区切るデータポイントを見つける。
- クラスタリングは、カテゴリがユーザーによって定義されないという点で分類とは異なる。
- たとえば、教師なしモデルは気温に基づいて気象データセットをクラスタ化し、季節を定義するセグメンテーションを明らかする。次に、データセットの理解に基づいて、これらのクラスタに名前を付ける。

## 強化学習モデル
- 環境内で実行されたアクションに基づいて報酬またはペナルティを取得することで予測を行う。
- 強化学習システムは、報酬を最大限に得るための最適な戦略を定義するポリシーを生成する。
- 強化学習は、部屋の中を歩くなどのタスクを実行するようにロボットをトレーニングしたり、AlphaGo などのソフトウェア プログラムで囲碁をプレイしたりするために使用される。

## 生成 AI
- ユーザー入力からコンテンツを作成するモデルの一種。
- たとえば、生成 AI は独自の画像、楽曲、ジョークを作成できる。また、記事の要約、タスクの実行方法の説明、写真の編集も可能。
- 生成 AI は、さまざまな入力を受け取り、テキスト、画像、音声、動画などのさまざまな出力を生成できる。また、これらの組み合わせを取得して作成することもできる。
- たとえば、モデルは画像を入力として受け取り、画像とテキストを出力として作成できる。また、画像とテキストを入力として受け取り、動画を出力として作成することもできる。
- 生成モデルは、入力と出力で説明できる。通常は「入力の種類」から「出力の種類」と表記されます。たとえば、次のリストは、生成モデルの入力と出力の一部を示している。
  - Text-to-text（テキストからテキスト）
  - Text-to-image（テキストから画像）
  - テキストからの動画生成
  - Text-to-code
  - テキスト読み上げ
  - 画像と Text-to-image
- 生成 AI の仕組み生成モデルは、新しいが類似したデータを生成することを目的として、データのパターンを学習する。
  - 人々の行動や話し方を観察して他人の真似を学ぶコメディアン
  - 特定のスタイルの絵画をたくさん研究して、そのスタイルで絵を描く方法を学ぶアーティスト
  - 特定の音楽グループの楽曲をたくさん聴いて、そのグループの音楽に似たサウンドを学ぶカバーバンド
- 独自で創造的な出力を生成するために、生成モデルは最初に教師なしアプローチを使用してトレーニングされる。
- このアプローチでは、モデルはトレーニングに使用したデータを模倣するように学習する。
- モデルは、記事の要約や写真の編集など、モデルに実行を依頼される可能性のあるタスクに関連する特定のデータに対して、教師あり学習または強化学習を使用してさらにトレーニングされることがある。
- 生成 AI は急速に進化しているテクノロジーで、新しいユースケースが絶えず発見されている。
- たとえば、生成モデルは、気を散らす背景を自動的に削除したり、低解像度の画像の品質を改善したりすることで、企業が e コマース商品画像を精査するのに役立つ。

----
# 教師ありの学習
教師あり学習のタスクは明確に定義されており、スパムの特定や降水量の予測など、さまざまなシナリオに適用できる。

## 教師あり学習の基本コンセプト
- データ
- モデル
- トレーニング
- 評価
- 推論

## データ
- データは ML の原動力。
- データは、テーブルに保存された単語や数値の形式で提供されるか、画像や音声ファイルでキャプチャされたピクセルや波形の値として提供される。
- 関連するデータはデータセットに保存される。
- データセットは、**特徴量**と**ラベル**を含む個々の例で構成される。
- スプレッドシートの 1 つの行に似ていると考えるとわかりやす。
- 特徴量は、教師ありモデルがラベルの予測に使用する値。
- ラベルは「解答」またはモデルに予測させる値。
- 降雨を予測する天気モデルでは、特徴として緯度、経度、気温、湿度、雲量、風向き、気圧などがあり、ラベルは「降水量」。

## データセットの特性
- データセットは、サイズと多様性によって特徴付けられる
- サイズはサンプル数。多様性とは、それらの例がカバーする範囲。
- 優れたデータセットは、大規模で多様性が高いもの。
- データセットによっては、サイズは大きいものの多様性が低いデータセットもあれば、サイズは小さいものの多様性が高いデータセットもある
- つまり、大規模なデータセットでも十分な多様性が保証されるわけではない。多様性の高いデータセットでも十分な例が保証されるわけではない。
- たとえば、データセットには 100 年間分のデータが含まれていても、7 月のデータのみが含まれている場合がある。
- このデータセットを使用して 1 月の降雨量を予測すると、予測の精度が低下します。
- 逆に、データセットは数年間のみを対象としているものの、すべての月が含まれている場合がある。
- このデータセットには変動を考慮するのに十分な年数が含まれていないため、予測の精度が低くなる可能性がある。

## モデル
- 教師あり学習では、モデルは特定の入力特徴パターンから特定の出力ラベル値への数学的な関係を定義する複雑な数値の集合。

## トレーニング
- 教師ありモデルが予測を行うには、トレーニングが必要。
- モデルをトレーニングするには、ラベル付きの例を含むデータセットをモデルに提供する。
- このモデルの目的は、特徴からラベルを予測するための最適なソリューションを導き出すこと
- デルは、予測値をラベルの実際の値と比較して、最適なソリューションを見つける。
- 予測値と実際の値の差（損失として定義）に基づいて、モデルはソリューションを徐々に更新する
- モデルは特徴とラベルの間の数学的な関係を学習し、未知のデータに対して最適な予測を行うことができる
- このようにして、モデルは特徴とラベルの間の正しい関係を徐々に学習する。
- この段階的な理解が、大規模で多様なデータセットから優れたモデルが生成される理由でもある。
- モデルは、値の範囲が広いより多くのデータを見て、特徴とラベルの関係をより正確に理解している。
- トレーニング中に、ML 担当者は、モデルが予測に使用する構成と特徴を微調整できる。

## 評価
- トレーニング済みモデルを評価して、学習の程度を判断する。
- モデルを評価する際は、ラベル付きデータセットを使用する。
- モデルにはデータセットの特徴のみを提供する。
- モデルの予測とラベルの真の値を比較します。

## 推論
- モデルの評価結果に満足したら、モデルを使用してラベルのないサンプルに対して予測（推論）を行うことができる。
- 天気アプリの例では、モデルに現在の気象条件（温度、気圧、相対湿度など）を入力すると、降水量が予測される。

----

# 線形回帰
- 線形回帰は、変数間の関連性を見つけるために使用される統計手法。
- ML のコンテキストでは、線形回帰は特徴量とラベルの関係を見つける。
- 例えば、自動車の重量に基づいて自動車の燃費（ガロンあたりのマイルで表す）を予測
- 代数学的に、このモデルは$`y=mx+b`$と定義
- ML では、線形回帰モデルの式は$`y'=b+w_1x_1`$
- ここで$`y'`$ は予測ラベル（出力）。
- $`b`$ はモデルのバイアス。バイアスはモデルのパラメータであり、トレーニング中に計算される。
- $`w_1`$は特徴の重み。重み付けは、線の代数方程式の勾配と同じ概念です。重みはモデルのパラメータであり、トレーニング中に計算される。
- $`x_1`$は特徴（入力）です。

----

# 線形回帰: 損失
- 損失は、モデルの予測の誤り度を示す数値指標。モデルの予測と実際のラベルの間の距離を測定する。
- モデルをトレーニングする目的は、損失を最小限に抑え、可能な限り低い値にすること。
- 損失は、値の方向ではなく、値の距離に焦点を当てる。
- モデルが 2 と予測し、実際の値が 5 の場合、損失が負であることは重要ではない。値の距離が 3 であることが重要
- 損失を計算するすべての方法で符号が削除される。
- 警告を削除する一般的な方法
  - 実際の値と予測値の差の絶対値を取得する
  - 実際の値と予測値の差を二乗する
 
# 損失の種類
- 主に 4 種類の損失がある。
  - L1 損失　予測値と実際の値の差の絶対値の合計
  - 平均絶対誤差（MAE）	サンプルセット全体の L1 損失の平均
  - L2 損失	予測値と実際の値の差の二乗の合計
  - 平均二乗誤差（MSE）	サンプルセット全体の L2 損失の平均
- L1 損失と L2 損失（または MAE と MSE）の機能的な違いは、平方。
- 予測とラベルの差が大きい場合、2 乗すると損失がさらに大きくなる
- 予測とラベルの差が小さい（1 未満）場合、2 乗すると損失はさらに小さくなる

# 損失の選択
- MAE と MSE のどちらを使用するかは、データセットと特定の予測を処理する方法によって異なる。
- 通常、データセット内のほとんどの特徴値は明確な範囲内に収まる
- 一般的な範囲外は、外れ値と見なされる。
- 最適な損失関数を選択する際は、モデルで外れ値をどのように処理するかを検討する。
- MSE はモデルを外れ値に近づけるが、MAE はそうしない。
- L2 損失では、L1 損失よりも外れ値に対するペナルティが大幅に大きくなる
- 外れ値は、MAE でトレーニングされたモデルよりも MSE でトレーニングされたモデルに近い
- モデルとデータの関係。
  - MSE。モデルは外れ値に近づくが、他のほとんどのデータポイントから離れる
  - MAE。モデルは外れ値から離れるが、他のほとんどのデータポイントに近づく

----

# 線形回帰: 勾配降下法
- 勾配降下は、損失が最も低いモデルを生成する重みとバイアスを反復的に見つけ出す数学的な手法。
- 勾配降下法は、ユーザー定義の反復処理を複数回繰り返して、最適な重みとバイアスを見つける。
- モデルは、ランダム化された重みとゼロに近いバイアスでトレーニングを開始し、次の手順を繰り返す
1. 現在の重みとバイアスを使用して損失を計算する。
1. 損失を減らす重みとバイアスの移動方向を決定する。
1. 損失を減らす方向に重みとバイアスの値を少しずつ移動しする。
1. ステップ 1 に戻り、モデルで損失をこれ以上削減できないまでこのプロセスを繰り返す。
- 重みとバイアスが更新されるたびに損失が低下している
- モデルは収束するまでトレーニングする。
- 勾配降下法によって損失をほぼ最小化する重みとバイアスが見つかる
- 追加で損失がさらに減少することはない
- モデルが収束後もトレーニングを継続すると、パラメータを最小値の周りで継続的に更新する
- 損失が少しずつ変動する。これにより、モデルが実際に収束したことを確認するのが難しくなる。
- モデルが収束したことを確認するには、損失が安定するまでトレーニングを続ける。
 
# モデルの収束と損失曲線
- モデルをトレーニングする際は、多くの場合、損失曲線を確認して、モデルが収束したかどうかを判断する。
- 損失曲線は、モデルのトレーニングに伴う損失の変化を示す。
- 一般的な損失曲線はY 軸が損失で、X 軸がイテレーション。

# 収束と凸関数
- 線形モデルの損失関数は常に凸なサーフェスを生成する。
- この特性により、線形回帰モデルが収束すると、損失が最も低くなる重みとバイアスがモデルによって検出されたことを意味する。

----
# 線形回帰: ハイパーパラメータ
- ハイパーパラメータは、トレーニングのさまざまな側面を制御する変数。
- 一般的なハイパーパラメータは学習率、バッチサイズ、エポック
- 一方、パラメータは、重みやバイアスなどの変数で、モデル自体の一部。
- つまり、ハイパーパラメータはユーザーが制御する値。パラメータはモデルがトレーニング中に計算する値。

# 学習率
- 学習率は、モデルの収束速度に影響する浮動小数点数。
- 学習率が低すぎると、モデルの収束に時間がかかることがある。
- ただし、学習率が高すぎると、モデルは収束せず、損失を最小化する重みとバイアスの周りをバウンスする。
- 目標は、モデルが迅速に収束するように、高すぎず低すぎない学習率を選択すること
- 学習率は、勾配降下プロセスの各ステップで重みとバイアスに加える変更の量を決定する。
- モデルは、勾配に学習率を乗算して、次の反復処理のモデルのパラメータ（重みとバイアス値）を決定する。
- 勾配降下法の 3 番目のステップで、負の勾配の方向に移動する「少量」は学習率を指す。
- 古いモデル パラメータと新しいモデル パラメータの差は、損失関数の勾配に比例する。
- たとえば、勾配が大きい場合、モデルは大きなステップを踏む。小さい場合は、小さなステップを踏む。
- 理想的な学習率は、妥当な数の反復処理でモデルを収束させるのに役立つ。

# バッチサイズ
- バッチサイズは、モデルが重みとバイアスを更新する前に処理するサンプルの数を指すハイパーパラメータ。
- 重みとバイアスを更新する前に、モデルはデータセット内のすべてのサンプルの損失を計算する必要があると思うかもしれない。
- データセットに数十万、数百万のサンプルが含まれている場合、完全なバッチを使用することは現実的ではない。
- データセット内のすべての例を確認することなく、平均で正しい勾配を取得する一般的な手法は 2 つ。
  - 確率的勾配降下法
  - ミニバッチ確率的勾配降下法
